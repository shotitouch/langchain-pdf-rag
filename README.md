# ğŸ“„ RAG PDF Chatbot â€” LangChain + FastAPI + Next.js

A full-stack **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDFs and ask questions grounded strictly in the documentâ€™s content.  
Built with **LangChain**, **FastAPI**, **OpenRouter**, **ChromaDB**, and a modern **Next.js** frontend.

This project demonstrates end-to-end GenAI engineering: ingestion, chunking, embedding, vector retrieval, LLM orchestration, and UI integration.

---

## ğŸ“Œ Deployment Status

This project currently runs **locally only**.  
There is **no hosted instance yet**.

Users must:

1. Run the FastAPI backend locally  
2. Run the Next.js frontend locally  
3. Upload their own PDF files to use the chatbot  

A future deployment on **Vercel (frontend)** + **Render (backend)** is planned.  
The README will be updated once hosting is available.

---

## ğŸš€ Features

- PDF ingestion â†’ extraction â†’ recursive chunking â†’ embeddings  
- RAG architecture with ChromaDB + OpenAI embeddings  
- FastAPI backend with modular routes and conversation memory  
- Next.js frontend with chat UI, file upload, and streaming responses  
- Strict context-only answering (hallucination mitigation)  
- Clean, production-ready code structure  

---

## ğŸ§± Tech Stack

### Backend
- FastAPI  
- LangChain  
- OpenRouter API  
- ChromaDB  
- Python 3.10+  

### Frontend
- Next.js (React + TypeScript)  
- TailwindCSS  
- Axios  

---

## ğŸ—ï¸ Architecture Overview

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Next.js UI  â”‚
            â”‚ (Chat + PDF  â”‚
            â”‚   Upload)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ FastAPI   â”‚
              â”‚  Backend  â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        Ingest PDF   â”‚   Chat Query
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ LangChain Pipeline   â”‚
       â”‚ - Text Splitter      â”‚
       â”‚ - Embeddings         â”‚
       â”‚ - Chroma Retrieval   â”‚
       â”‚ - ChatOpenAI         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ChromaDB      â”‚
        â”‚ Vector Store    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¥ Backend Setup (FastAPI)

### 1. Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate      # Mac/Linux
.venv\Scripts\activate         # Windows
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Environment variables
Create `.env`:

```
OPENROUTER_API_KEY=your_key_here
```

### 4. Start backend
```bash
uvicorn app.main:app --reload
```

Runs at: **http://localhost:8000**

---

## ğŸ“¤ API Endpoints

### POST /ingest â€” Upload & store PDF
```bash
curl -X POST -F "file=@example.pdf" http://localhost:8000/ingest
```

Response:
```json
{
  "message": "File ingested successfully",
  "chunks": 128
}
```

---

### POST /chat â€” Ask questions about ingested PDF
Request:
```json
{
  "question": "What is the main purpose of this document?"
}
```

Response:
```json
{
  "answer": "The document discusses ..."
}
```

---

## ğŸ’» Frontend Setup (Next.js)

### 1. Install dependencies
```bash
npm install
```

### 2. Environment variables  
Create `.env.local`:

```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

### 3. Start development server
```bash
npm run dev
```

Runs at: **http://localhost:3000**

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ retriever/
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ models/
â”‚   â””â”€â”€ chroma/
â”‚
â””â”€â”€ client/
    â”œâ”€â”€ pages/
    â”œâ”€â”€ components/
    â”œâ”€â”€ styles/
    â””â”€â”€ utils/
```

---

## ğŸ§  How RAG Works

1. User uploads a PDF  
2. Text extracted using PyPDFLoader  
3. Split into chunks with `RecursiveCharacterTextSplitter`  
4. Embeddings generated via OpenAI / OpenRouter  
5. Stored in ChromaDB for retrieval  
6. Retriever selects relevant chunks per question  
7. ChatOpenAI generates an answer using **context only**  
8. Frontend streams answer to UI  

---

## ğŸ”’ Hallucination Control

```
Use ONLY the provided context.
If the answer is not found, reply:
"Not found in context."
```

---

## ğŸ§ª Testing Tips

Try:

- â€œSummarize this PDF.â€  
- â€œWhat does section 3 say?â€  
- â€œList all dates mentioned.â€  
- â€œWhat is the main argument?â€  

---

## ğŸš€ Deployment (Future Plan)

- Frontend â†’ **Vercel**  
- Backend â†’ **Render / Railway / AWS EC2**  
- Persistent Chroma storage  
- Production CORS configuration  

---

## ğŸ“œ License
MIT License

---

## â­ Acknowledgements
Powered by: LangChain, FastAPI, OpenRouter, ChromaDB, Next.js  
